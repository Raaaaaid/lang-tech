\documentclass[a4paper,12pt,numbers=enddot]{scrartcl}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2cm,bottom=2.5cm]{geometry}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{cite}
\usepackage[square,numbers]{natbib}

% Settings
\bibliographystyle{dinat}
\setlength\parindent{0pt}
\thispagestyle{fancy}

% Header and Footer
\fancyhead[LE,LO]{19th June 2022}
\fancyhead[RE,RO]{Exposé}
\fancyhead[CE,CO]{Group: \glqq We're not quite sure what we're doing\grqq}
\fancyfoot{}

\begin{document}

\singlespacing

\begin{Large}
\begin{center}
\textbf{Explicit Sentiment Analysis with Language Patterns about Uncertainty}
\end{center}
\end{Large}

Our goal with this project is divided into two parts. First of all, we want to extract a dataset about semantic uncertainty from the web archive data \citep{Kiesel2018}. This dataset is then compared to (welcher Vergleichsdatensatz?) to clarify how well our dataset conforms with this popular sentiment classification dataset. In the second part we train a classifier (welches modell?) on our dataset using transfer learning and benchmark this model on the previous mentioned dataset to see if our dataset is suited to train sentiment classification. 
\\

We will use certain language patterns about uncertainty to extract samples from web archive data. An overview of the patterns can be found here \citep[p. 43]{vincze2014uncertainty}. To classify these samples into positive/negative sentiments we will mainly use GPT-3. Because we cannot fully trust these results based on GPT-3, we will verify at least some of the labels manually. We plan to analyse our dataset based on what topics the internet is most uncertain about - possibly even how those topics changed over time (wie heißt dein schönes chart pascal?). After the labeling process we want to compare strong positive/negative topics of our data with those of the dataset mentioned above (wenn das bei dem datensatz überhaupt sinn macht?!). So the deliverables from this step are the dataset and visualizations of the comparison.
\\

In the next step we train our model. We prevent train-test leakage with the known train-validation-test split method. The ratios of this split depends on the amount of samples we actually get in the end. We then compare the performance of our model to other models (Quelle mit Ergebnissen von anderen modellen zu diesem datensatz) based on the dataset from above. With these evaluation results we can determine if our data can be used to train sentiment classification.
\\

We distribute our project in the following three work packages:
\begin{itemize}
	\setlength\itemsep{-5pt}
	\item dataset extraction from the web archive data
	\item analyzing, visualising and labeling the dataset and comparing it to (name anderer datensatz)
	\item train a model and evaluate it on said dataset
\end{itemize}
\medskip

\hrule
\vspace{-2.5em}
\renewcommand{\refname}{}
\nocite{*}
\bibliography{literature}

\clearpage

\begin{itemize}
	\setlength\itemsep{-5pt}
	\item[x] What is your research goal/the research question you are trying to answer?
	\item[x] Is there any previous/related work?
	\item[x] What are the necessary steps (research plan)? How will those be distributed among 	team members?
	\item[x] What data will you use? What preprocessing steps are necessary? Do you need to annotate the data? How will this be done? Manually? With the assistance of e.g. GPT-3?
	\item[x] What will be the train-validation-test split? How do you prevent train-test leakage?
	\item[x] Are there baseline datasets or baseline methods for your approach that you will compare your results to?
	\item[x] Will you train a new model? Will you use an existing model architecture/a pretrained model? Finetuning, head retraining?
	\item What experiments will you conduct? What will be the evaluation methods/quality criteria/metrics?
	\item[x] How will you be able to answer the research question based on those experiments?
\end{itemize}

\begin{itemize}
	\setlength\itemsep{-5pt}
	\item[x] The cleaned up/refined dataset (if you release data, provide a datasheet https://arxiv.org/abs/1803.09010)
	\item[(x)] The trained model (if you release a model, provide a model card https://arxiv.org/abs/1810.03993)
	\item[(x)] For shared tasks: A task description and evaluation scripts
	\item[x] Evaluation results and plots
	\item[x] Visualizations and/or interactive demonstrations (if applicable - specify what and in which suitable format)
\end{itemize}

\end{document}